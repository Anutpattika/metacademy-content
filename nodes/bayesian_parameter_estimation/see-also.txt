* Ultimately, we don't just want to learn parameters, we want to use them for something. Bayesian decision theory concerns how to act based on our inferences from the data. [bayesian-decision-theory]
* Usually, the posterior over parameters and the predictive distribution can't be computed in closed form. Here are some strategies for getting approximate solutions:
** maximum a posteriori (MAP) estimation, i.e. finding the most likely parameters [map-parameter-estimation]
** variational Bayes, a framework for approximating intractable posterior distributions with tractable ones [variational-bayes]
** markov chain Monte Carlo (MCMC), a set of techniques for approximately sampling from the posterior [markov-chain-monte-carlo]
* Regularization can often be viewed as MAP estimation in a Bayesian model. [regularization-as-map]
* While Bayesian parameter estimation attenuates overfitting, it doesn't solve the problem completely. Strategies for controlling model complexity include:
** Bayesian model comparison [bayesian-model-comparison]
** Bayesian model averaging [bayesian-model-averaging]
** Bayesian nonparametrics [bayesian-nonparametrics]
* The choice of a prior distribution over parameters is not always obvious. Often we choose them based on criteria like the following:
** conjugate priors, where the prior and posterior have the same functional form [conjugate-priors]
** uninformative priors, which try to say as little as possible about the parameter [uninformative-priors]
** Bayesian model averaging, where we average over multiple choices of the prior [bayesian-model-averaging]
** hierarchical models, where the prior for one problem comes from information obtained from related problems [hierarchical-bayesian-modeling]
** eliciting priors from human experts [eliciting-priors]
