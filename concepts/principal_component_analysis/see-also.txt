* "Mathematical justification":pca_proof of PCA
* Some ways PCA is commonly used:
** to "visualize datasets":pca_visualization by projecting into a low-dimensional space
** "as a preprocessing step":pca_preprocessing for supervised learning; the idea is to improve generalization or computational efficiency by reducing the dimensionality of the inputs
** "latent semantic analysis (LSA)":latent_semantic_analysis, a way of uncovering topics from text
* Some generalizations of PCA and related models:
** "probabilistic PCA":probabilistic_pca, where the same algorithm is interpreted as fitting a generative model
** "factor analysis":factor_analysis, another related generative model where each input dimension can have a separate noise variance
** "Bayesian PCA":bayesian_pca
** "probabilistic matrix factorization (PMF)":probabilistic_matrix_factorization, a PCA-like model for predicing missing entries of a matrix
** "kernel PCA":kernel_pca, which implicitly maps the data to a high-dimensional space before computing the PCA vectors
* For very high-dimensional spaces, directly computing the leading eigenvectors is impractical. In these situations, "expectation-maximization (EM)":em_for_pca can be more practical.
* "Fisher's linear discriminant":fishers_linear_discriminant is another projection similar to PCA, but which uses class labels also.

