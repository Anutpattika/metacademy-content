* "Natural gradient":natural_gradient allows us to speed up stochastic gradient descent by accounting for the curvature of the objective function.
* "Second-order optimization methods":second_order_optimization more generally often converge faster than first order methods, though they are harder to do in a stochastic framework.
* In "large-scale machine learning":tradeoffs_large_scale_learning, stochastic gradient descent achieves a good tradeoff between statistical error and optimization error.

