* Some other parameter estimation methods include:
** the "method of moments":method_of_moments
** "maximum likelihood":maximum_likelihood
* Ultimately, we don't just want to learn parameters, we want to use them for something. "Bayesian decision theory":bayesian_decision_theory concerns how to act based on our inferences from the data.
* Usually, the posterior over parameters and the predictive distribution can't be computed in closed form. Here are some strategies for getting approximate solutions:
** "maximum a posteriori (MAP) estimation":map_parameter_estimation, i.e. finding the most likely parameters
** "variational Bayes":variational_bayes, a framework for approximating intractable posterior distributions with tractable ones
** "markov chain Monte Carlo (MCMC)":markov_chain_monte_carlo, a set of techniques for approximately sampling from the posterior
* Regularization can often be "viewed as MAP estimation in a Bayesian model":regularization_as_map.
* While Bayesian parameter estimation attenuates overfitting, it doesn't solve the problem completely. Strategies for controlling model complexity include:
** "Bayesian model comparison":bayesian_model_comparison
** "Bayesian model averaging":bayesian_model_averaging
** "Bayesian nonparametrics":bayesian_nonparametrics
* The choice of a prior distribution over parameters is not always obvious. Often we choose them based on criteria like the following:
** "conjugate priors":conjugate_priors, where the prior and posterior have the same functional form
** "uninformative priors":uninformative_priors, which try to say as little as possible about the parameter
** "Bayesian model averaging":bayesian_model_averaging, where we average over multiple choices of the prior
** "hierarchical models":hierarchical_bayesian_modeling, where the prior for one problem comes from information obtained from related problems [hierarchical-bayesian-modeling]
** "eliciting priors from human experts":eliciting_priors
